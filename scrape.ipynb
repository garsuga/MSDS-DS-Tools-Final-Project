{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS DS Tools Final Project: Topic Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook approaches topic clustering and visualization using a few processes:\n",
    "* Articles are scraped from Times.com\n",
    "* Article content is summarized using `sentencetransformers`\n",
    "* Articles are clustered by summary using `sentencetranformers` again\n",
    "* Clusters are tagged with a human-readable topic using LLama 3.1\n",
    "* Results are visualized as a network graph in `pyvis`\n",
    "\n",
    "This process worked surprisingly well and the visualization provides an at-a-glance view of the latest Times.com content.\\\n",
    "More information is available below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "> **Note:** Some of these requirements are time-consuming/difficult to set up, they can be skipped by loading data from the PKL files\n",
    "#### General / Web Scraping\n",
    "* `ipywidgets`: notebook tools\n",
    "* `pyquery`: HTML parsing\n",
    "* `tqdm`: progress bars\n",
    "* `pyvis`: graph visual\n",
    "* `matplotlib`: coloring\n",
    "* `nltk`: NLP tools\n",
    "* `gensim`: NLP tools\n",
    "#### For Basic Topic Grouping (Skippable)\n",
    "* `sentencetransformers`: basic semantic models, clustering, summarizing\n",
    "* `pytorch`: GPU processing for `sentencetransformers`\n",
    "* `CUDA`, `CUDNN`, `Visual Studio + MSVC`: for `pytorch` GPU capabilities\n",
    "### For Topic Defining (Skippable)\n",
    "* `ollama`: LLM communication\n",
    "* `llama3.1-70b`: LLM choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General / Topic Grouping Dependencies Installs\n",
    "> See above for information on these dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install ipywidgets\n",
    "%pip install pyquery\n",
    "%pip install tqdm\n",
    "%pip install nltk\n",
    "%pip install pyvis\n",
    "%pip install matplotlib\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions And Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "import asyncio\n",
    "from typing import Any, AsyncGenerator, List, Set, Tuple, Dict\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## Constants\n",
    "TIMES_URL = \"https://time.com/\"\n",
    "TIMES_HOME_URL = urljoin(TIMES_URL, \"section/us/\")\n",
    "TIMES_PAGE_URL = \"/section/us/?page={page}\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:128.0) Gecko/20100101 Firefox/128.0\"\n",
    "}\n",
    "N_ARTICLES = 20\n",
    "SUMMARIZED_ARTICLES_PICKLE = \"summarized_articles.pkl\"\n",
    "ARTICLES_CORPUS_PICKLE = \"articles_with_corpus.pkl\"\n",
    "ARTICLES_PICKLE = \"articles.pkl\"\n",
    "ARTICLES_TOPIC_PICKLE = \"articles_with_topic.pkl\"\n",
    "CLUSTERS_PICKLE = \"clusters.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_str(cls: type):\n",
    "    \"\"\"\n",
    "    Decorates a class with a `__str__` and `__repr__` function that returns a JSON representation of the object\n",
    "    \"\"\"\n",
    "    def custom_str(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        properties = {k: v for k, v in self.__dict__.items()}\n",
    "        return json.dumps({\"class\": class_name, \"properties\": properties}, indent=4)\n",
    "    cls.__str__ = custom_str\n",
    "    cls.__repr__ = custom_str\n",
    "    return cls\n",
    "\n",
    "def remove_unicode_and_normalize(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes unicode characters to ascii or removes them otherwise\n",
    "    \"\"\"\n",
    "    normalized = unicodedata.normalize('NFKD', s)\n",
    "    return normalized.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@json_str\n",
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Shared rate limiter for web requests\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int=1, interval:float=2):\n",
    "        \"\"\"\n",
    "        Creates a simple `RateLimiter` that allows for `n` requests in `interval` seconds.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.interval = interval\n",
    "        self.calls = 0\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self) -> None:\n",
    "        \"\"\"\n",
    "        Waits for next available call, concurrency-safe\n",
    "        \"\"\"\n",
    "        async with self.lock:\n",
    "            if self.calls >= self.n:\n",
    "                await asyncio.sleep(self.interval)\n",
    "                self.calls = 0\n",
    "            self.calls += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "\n",
    "T = TypeVar('T')\n",
    "async def async_islice(generator: AsyncGenerator[T, None], stop: int, start: int = 0) -> AsyncGenerator[T, None]:\n",
    "    \"\"\"\n",
    "    Async-compatible implementation of `itertools.islice`\n",
    "    \"\"\"\n",
    "    i = -1\n",
    "    async for value in generator:\n",
    "        i += 1\n",
    "        if i < start:\n",
    "            continue\n",
    "        if i >= stop:\n",
    "            break\n",
    "        yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extensible article definitions\n",
    "\n",
    "@json_str\n",
    "class Article:\n",
    "    def __init__(self, url: str, title: str, paragraphs: List[str]):\n",
    "        self.paragraphs: List[str] = paragraphs\n",
    "        self.title: str = title\n",
    "        self.url: str = url\n",
    "\n",
    "@json_str\n",
    "class SummarizedArticle(Article):    \n",
    "    def __init__(self, summary: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.summary: str = summary\n",
    "\n",
    "@json_str\n",
    "class ArticleWithCorpus(SummarizedArticle):\n",
    "    def __init__(self, corpus: List[str], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.corpus : List[str] = corpus\n",
    "\n",
    "@json_str\n",
    "class ArticleWithTopic(ArticleWithCorpus):\n",
    "    def __init__(self, topic: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.topic: str = topic\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "Web scraping is done with a asyncronous iterator. Pages are iterated directly using URLs and articles are iterated from each page. These processes join on an iterator of `Article`'s and obeys rate limiting as its needed. `Article`'s are only retrieved as they are iterated.\\\n",
    "Times.com is relatively easy to scrape due to it having lists of article on each page of its US category and an iterable URL structure in the form `/section/us/?page={page}`\\\n",
    "Rate-limiting is done as one request per second and `500` articles are grabbed by default. This takes a bit of time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def iter_times(max_pages: int = 10) -> AsyncGenerator[Article, None]:\n",
    "    \"\"\"\n",
    "    Returns an `AsyncGenerator` that yields `Article` instances. Articles are retrieved as they are iterated and rate limiting happens with the flow of the iterator\n",
    "    \"\"\"\n",
    "    rl = RateLimiter(interval=1)\n",
    "\n",
    "    def get_page_from_url(url) -> pq:\n",
    "        \"\"\"\n",
    "        Creates a `PyQuery` instance for the page indicated by `url`\n",
    "        \"\"\"\n",
    "        resp = requests.get(url, headers=HEADERS)\n",
    "        if resp.status_code != 200:\n",
    "            return None\n",
    "        page = pq(resp.text)\n",
    "        return page\n",
    "    \n",
    "    async def iter_pages(start_url: str) -> AsyncGenerator[pq, None]:\n",
    "        \"\"\"\n",
    "        Returns an `AsyncGenerator` of pages, returns a `PyQuery` instance for each page\n",
    "        \"\"\"\n",
    "        page_num = 1\n",
    "        \n",
    "        def get_next_page() -> pq:\n",
    "            \"\"\"\n",
    "            Returns a `PyQuery` instance for the next page by determining the URL from the next index\n",
    "            \"\"\"\n",
    "            nonlocal page_num\n",
    "            page_num += 1\n",
    "            page = get_page_from_url(urljoin(TIMES_URL, TIMES_PAGE_URL.format(page=page_num)))\n",
    "            return page\n",
    "        \n",
    "        await rl.acquire()\n",
    "        page = get_page_from_url(start_url)\n",
    "        await rl.acquire()\n",
    "        next_page = get_next_page()\n",
    "        yield page\n",
    "\n",
    "        while next_page is not None:\n",
    "            yield page\n",
    "            await rl.acquire()\n",
    "            page = next_page\n",
    "            next_page = get_next_page()\n",
    "\n",
    "    def get_article_urls(head_page: pq) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of article urls from the given page (`PyQuery`)\n",
    "        \"\"\"\n",
    "        article_urls = [x.attr(\"href\") for x in head_page(\"div.component div.taxonomy-tout a\").items()]\n",
    "        return article_urls\n",
    "    \n",
    "    def get_article_from_url(url: str) -> Article:\n",
    "        \"\"\"\n",
    "        Creates an `Article` instance for the article page `url` provided.\n",
    "        \"\"\"\n",
    "        page = get_page_from_url(url)\n",
    "        try:\n",
    "            title = remove_unicode_and_normalize(page(\"h1.self-baseline\").text())\n",
    "            paragraphs = [remove_unicode_and_normalize(x.text()) for x in page(\"p.self-baseline.px-0\").items()]\n",
    "            return Article(url, title, paragraphs)\n",
    "        except Exception as e:\n",
    "            print(f\"Article at url='{url}' failed to parse: {e}\")\n",
    "            return None\n",
    "\n",
    "    async for page in async_islice(iter_pages(TIMES_HOME_URL), max_pages):\n",
    "        urls = get_article_urls(page)\n",
    "        for url in urls:\n",
    "            yield get_article_from_url(urljoin(TIMES_URL, url))\n",
    "            await rl.acquire()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "In order to clean the text, the following processes are used:\n",
    "* Removing numbers\n",
    "* Removing stopwords\n",
    "* Lemmatizing tokens\n",
    "* Removing words used only a single time\n",
    "* Removing named-entities (ex: Trump, Washington)\n",
    "* Lowercases words\n",
    "\n",
    "The token list is not used for determining the final topic but is instead used to summarize and cluster articles. By removing named entities and infrequent words the models in `sentencetransformers` are less prone to confusion with unfamiliar or unimportant tokens.\\\n",
    "These tokens are otherwise mainly used during visualization where named entities clutter the screen without displaying meaningful connections between topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "#Implementation from https://stackoverflow.com/questions/43742956/fast-named-entity-removal-with-nltk\n",
    "def fast_ne_removal(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Removes named-entities from the given token list\n",
    "    \"\"\"\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged)\n",
    "    non_entities = [leaf[0] for leaf in tree if type(leaf) != nltk.Tree]\n",
    "    return non_entities\n",
    "\n",
    "def clean_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans the text provided and tokenizes using the following processes:\n",
    "    * Removes digits\n",
    "    * Removes stopwords\n",
    "    * Lemmatizes tokens\n",
    "    * Removes words used only a single time\n",
    "    * Removes named-entities\n",
    "    * Lowercases words\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    words = [word for word in words if freq_dist[word] > 1]\n",
    "    \n",
    "    words = fast_ne_removal(words)\n",
    "    words = [s.lower() for s in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloration\n",
    "The final number of topics is uncertain so ChatGPT helped make a function that could combine and wrap colorsets from `matplotlib`. These are converted to hex before being used in visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import rgb2hex\n",
    "\n",
    "#chatgpt\n",
    "def get_combined_colormap(N):\n",
    "    \"\"\"\n",
    "    Returns a list of colors of length `N` in hex format by wrapping through multiple colorsets\n",
    "    \"\"\"\n",
    "    colormaps = ['tab20', 'tab20c', 'tab10', 'Set3']  # A combination of discrete colormaps\n",
    "    colors = []\n",
    "\n",
    "    for cmap_name in colormaps:\n",
    "        cmap = plt.cm.get_cmap(cmap_name)\n",
    "        colors.extend([rgb2hex(c) for c in cmap.colors])\n",
    "\n",
    "    return colors[:N] if N <= len(colors) else colors * (N // len(colors)) + colors[:N % len(colors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Checkpoint\n",
    "> **Note**: All of the code above is common definitions. After running those, run the cell below and skip to visualization if getting new data is not necessary\n",
    "\n",
    "The data in this notebook can be very time-consuming to obtain. Any time a long process is used, the output is saved to a PKL file so that it can be restored.\\\n",
    "If there are any parts of the code that cannot be run on the current system, the provided PKL files can be used to load the data needed for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles.pkl: SUCCESS\n",
      "summarized_articles.pkl: SUCCESS\n",
      "articles_with_corpus.pkl: SUCCESS\n",
      "clusters.pkl: SUCCESS\n",
      "articles_with_topic.pkl: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def read_pickle(filename: str) -> Any:\n",
    "    \"\"\"\n",
    "    Read pickle and print status\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            print(f\"{filename}: SUCCESS\")\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        print(f\"{filename}: MISSING\")\n",
    "        return None\n",
    "\n",
    "\n",
    "articles: List[Article] = read_pickle(ARTICLES_PICKLE)\n",
    "summarized_articles: List[SummarizedArticle] = read_pickle(SUMMARIZED_ARTICLES_PICKLE)\n",
    "articles_corpus: List[ArticleWithCorpus] = read_pickle(ARTICLES_CORPUS_PICKLE)\n",
    "clusters: List[List[int]] = read_pickle(CLUSTERS_PICKLE)\n",
    "articles_topic: List[ArticleWithTopic] = read_pickle(ARTICLES_TOPIC_PICKLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Articles (Skippable)\n",
    "This code collects the articles this notebook operates on. By default `500` articles are collected.\\\n",
    "This was done relatively easily and there are no findings at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267d2083894a48f4b0408465b871d499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/500 before iterator ended.\n"
     ]
    }
   ],
   "source": [
    "N_ARTICLES = 500\n",
    "\n",
    "articles: List[Article] = []\n",
    "with tqdm(total=N_ARTICLES) as pbar:\n",
    "    async for article in async_islice(iter_times(max_pages=100), N_ARTICLES):\n",
    "        pbar.update(1)\n",
    "        if article is not None:\n",
    "            articles.append(article)\n",
    "    tqdm.write(f\"Processed {len(articles)}/{N_ARTICLES} before iterator ended.\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(ARTICLES_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing (Skippable)\n",
    "This code uses one of my favorite implementations in the `sentencetransformers` examples. It is a version of `LexRank` which will be used to rank and extract top sentences from the articles.\\\n",
    "This is a form of summarizing that does not require generating new text and has low risk of adding false data.\\\n",
    "The summaries are saved to the extended `Article` class `SummarizedArticle`\n",
    "\n",
    "These summaries will be used to check work and be passed into a second `LexRank` process later to summarize topics. Doing the second `LexRank` process this way maintains the importance of individual articles while summarizing topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install for `sentencetransformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sentencetransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9efb99e2ca4f4b8a860461be1ffe24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 499 articles.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "# https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/text-summarization\n",
    "from LexRank import degree_centrality_scores\n",
    "\n",
    "SUMMARY_LENGTH = 3\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "summarized_articles: List[SummarizedArticle] = []\n",
    "\n",
    "with tqdm(total=len(articles)) as pbar:\n",
    "    for article in articles:\n",
    "        sentences: List[str] = []\n",
    "        for paragraph in article.paragraphs:\n",
    "            if paragraph is not None and len(paragraph) > 0:\n",
    "                sentences.extend([tok for tok in sent_tokenize(paragraph) if len(tok.strip()) > 0])\n",
    "\n",
    "        sentences = np.array(sentences)\n",
    "\n",
    "        if len(sentences) < 1:\n",
    "            continue\n",
    "        \n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/text-summarization/text-summarization.py\n",
    "        similarity_scores = model.similarity(embeddings, embeddings).numpy()\n",
    "        centrality_scores = degree_centrality_scores(similarity_scores, threshold=None)\n",
    "        most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "\n",
    "        summary = \"\\n\".join(sentences[most_central_sentence_indices[0:SUMMARY_LENGTH]])\n",
    "        summarized_articles.append(SummarizedArticle(summary, **article.__dict__))\n",
    "        pbar.update(1)\n",
    "    pbar.write(f\"Summarized {len(summarized_articles)} articles.\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(SUMMARIZED_ARTICLES_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(summarized_articles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summaries show that key points are being addressed from each article. These seem intuitive and the data is ready for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iran Is Trying to Interfere in U.S. Election, Including Hacking Campaigns: Intel Agencies:\n",
      "\tBesides breaching the Trump campaign, officials also believe that Iran tried to hack into the presidential campaign of Kamala Harris.\n",
      "We have observed increasingly aggressive Iranian activity during this election cycle, specifically involving influence operations targeting the American public and cyber operations targeting Presidential campaigns, said the statement released by the FBI, the Office of the Director of National Intelligence and the Cybersecurity and Infrastructure Security Agency.\n",
      "Earlier this month, Microsoft issued a report detailing foreign agents attempts to interfere in this years election, citing an instance of an Iranian military intelligence unit in June sending a spear-phishing email to a high-ranking official of a presidential campaign from a compromised email account of a former senior advisor.\n",
      "\n",
      "The Future Is Here: The Biggest Moments From Hillary Clintons 2024 DNC Speech:\n",
      "\tAs the Democratic Party finds itself once again on the cusp of potentially electing its first female President, Clinton, now 76, used her speech to rally behind Vice President Kamala Harris, salute President Joe Biden, and remind Democrats of the progress made and the opportunities still within reach since her historic presidential run in 2016.\n",
      "Eight years ago, Hillary Clinton addressed the Democratic National Convention as the first woman to lead a major partys presidential ticket, hoping to break the proverbial glass ceiling.\n",
      "Clinton spoke with a blend of nostalgia and optimism, recounting the challenges and breakthroughs of her 2016 campaign and framing Harris campaign as the latest chapter in a continuing historical struggle for gender equality in American politics.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([f\"{s.title}:\\n\\t{s.summary}\" for s in summarized_articles[0:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Clean Corpuses For Each Article (Skippable)\n",
    "The content of each article is passed into the text cleaning process described above. These clean corpuses are saved to the extended `Article` class `ArticleWithCorpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_corpus: List[ArticleWithCorpus] = [ArticleWithCorpus(clean_text(\"\\n\".join(s.paragraphs)), **s.__dict__) for s in summarized_articles]\n",
    "\n",
    "import pickle\n",
    "with open(ARTICLES_CORPUS_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(articles_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Clustering (Skippable)\n",
    "Using `sentencetransformers` again, the articles can be clustered by their embeddings by detecting \"communities\". This does not actually decide a description of the topic as this is a much trickier process.\\\n",
    "Usually, getting named topics involves defining or seeding topics for a model to look for. This may have been possible here but the intention was for the process to be as unguided as possible and seeding topics can cause a bias in the modelling.\\\n",
    "These untagged clusters will get tagged later with the help of a more modern approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4974504a2b584c6d88ead18718ddb761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode([\" \".join(a.corpus) for a in articles_corpus], batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "clusters: List[List[int]] = util.community_detection(embeddings, min_community_size=3, threshold=0.5)\n",
    "\n",
    "import pickle\n",
    "with open(CLUSTERS_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters can be seen below and each cluster appears related according to human intuition. Next, tagging the clusters needs to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) 37 Articles\n",
      "\t \n",
      "\t How Republicans Are Reacting to the Colorado Ruling to Remove Trump From the Ballot\n",
      "\t Read the Full Transcripts of Donald Trumps Interviews With TIME\n",
      "\t...\n",
      "(2) 35 Articles\n",
      "\t What Americas Student Photojournalists Saw at the Campus Protests\n",
      "\t Columbias Relationship With Student Protesters Has Long Been Fraught\n",
      "\t Gaza Calls, Columbia Falls: Campus Protesters Defy Suspension Threats and Occupy Hall\n",
      "\t...\n",
      "(3) 12 Articles\n",
      "\t High Debt Threatens the U.S. Economy\n",
      "\t The Republican War on Food Programs\n",
      "\t Is America in Decline?\n",
      "\t...\n"
     ]
    }
   ],
   "source": [
    "for i, cluster in enumerate(clusters[0:3]):\n",
    "    print(f\"({i + 1}) {len(cluster)} Articles\")\n",
    "    for sentence_id in cluster[0:3]:\n",
    "        print(\"\\t\", articles_corpus[sentence_id].title)\n",
    "    print(\"\\t...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Tagging With LLama 3.1 70B (Skippable)\n",
    "> **Note**: This is a time and resource intensive process, even with ~50 clusters. It is recommend to load this from the checkpoint and skip this.\n",
    "\n",
    "In the interest of cost, a local Ollama instance was used instead of an API. Due to the limitations and performance of LLMs, it was necessary to condense the information in the articles to a manageable amount before reaching this step.\\\n",
    "LLMs perform best when the fewest unnecessary tokens are used. To start, each summary of each article in a cluster is tokenized by sentence and added to a central list. `LexRank` is used again to pick the top `10` sentences from this list to form a cluster summary.\\\n",
    "Now, the data is ready for LLama to determine the shared topic. The system prompt used can be seen below and is used to create a specialized LLama instance in Ollama. Each cluster summary is passed to the LLM which is tasked with finding a couple of words to describe the similarities. This takes quite a bit of time and computing power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install for Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the specialized LLama 3.1 70B instance in Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "modelspec='''\n",
    "FROM llama3.1:70b\n",
    "SYSTEM You will be given a number of article summaries you need to find the common topic between. This topic should only be a couple of words at a maximum and there should only be one topic. Make sure the topic corresponds to EVERY summary provided. If you cannot find a topic respond with only NONE. Summaries are separated by newlines. Respond only with the short topic as described with no extra information.\n",
    "PARAMETER temperature 0.0\n",
    "'''\n",
    "\n",
    "ollama.create(model='llama3.1-70-topic', modelfile=modelspec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag topics for each cluster with the specialized LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaf92e0b7df40e3b935bdad6b79fda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing Clusters:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 54 clusters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196ce2eba1344851ad7642a43198bd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting topics:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "# https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/text-summarization\n",
    "from LexRank import degree_centrality_scores\n",
    "\n",
    "topic_sentences: List[List[str]] = []\n",
    "for cluster in clusters:\n",
    "    sentences: List[str] = []\n",
    "    for idx in cluster:\n",
    "        sentences.extend(sent_tokenize(articles_corpus[idx].summary))\n",
    "    topic_sentences.append(sentences)\n",
    "\n",
    "TOPIC_SUMMARY_LENGTH = 10\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "summaries: List[str] = []\n",
    "with tqdm(total=len(clusters), desc=\"Summarizing Clusters\") as pbar:\n",
    "    for i, sentences in enumerate(topic_sentences):\n",
    "        sentences = np.array(sentences)\n",
    "        if len(sentences) < 1:\n",
    "            continue\n",
    "        \n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/text-summarization/text-summarization.py\n",
    "        similarity_scores = model.similarity(embeddings, embeddings).numpy()\n",
    "        centrality_scores = degree_centrality_scores(similarity_scores, threshold=None)\n",
    "        most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "\n",
    "        summary = \"\\n\\n\".join(sentences[most_central_sentence_indices[0:TOPIC_SUMMARY_LENGTH]])\n",
    "        summaries.append(summary)\n",
    "        pbar.update(1)\n",
    "    pbar.write(f\"Summarized {len(summaries)} clusters.\")\n",
    "\n",
    "articles_topic: List[ArticleWithTopic] = []\n",
    "with tqdm(total=len(summaries), desc=\"Extracting topics\") as pbar:\n",
    "    for i, summary in enumerate(summaries):\n",
    "        topic = ollama.generate(model=\"llama3.1-70-topic\", prompt=summary)['response']\n",
    "        cluster = clusters[i]\n",
    "        \n",
    "        for sentence_id in cluster:\n",
    "            articles_topic.append(ArticleWithTopic(topic, **articles_corpus[sentence_id].__dict__))\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "import pickle\n",
    "with open(ARTICLES_TOPIC_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(articles_topic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Graph Visualization (Skippable)\n",
    "> **Note**: The visualization is output as an HTML file. Since HTML cannot be easily loaded into VSCode it is not running in Notebook mode. Open [net.html](./net.html) in your browser to view the graph.\n",
    "\n",
    "The bulk of the results is seen in the visual generated below. It uses `pyvis` to create a graph of topics and their connected keywords.\\\n",
    "At this step, there were ~5300 unique keywords from the 500 articles so a lot of extra cleaning is needed. Many words are either too generic or specific to be meaningful. Additionally words need to be important to the topic. If a word is not present enough in the cluster it is not guaranteed to be significant. Likewise if a word is present in too many topics it is not meaningful enough to include.\\\n",
    "\n",
    "The process for this keyword filtering is as follows:\n",
    "* A word needs to be present in at least 75% of articles in the cluster\n",
    "* A word needs to be present in at most 3 clusters\n",
    "\n",
    "This helps guarantee that the words are not too infrequent, not too specfic, and not too generic. These conditions were tuned iteratively based on the visual.\\\n",
    "\n",
    "Each topic is graphed as a node with its size according to the number of related articles. A colored edge is formed to the filtered keywords associated with this. Each keyword node is subtly sized based on its overall frequency. Topics with at least one shared word are ancestors. The rest is much easier seen in the visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install for `pyvis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pyvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words=5081\n",
      "Mean Word Frequency Overall=178.82565130260522\n",
      "Median Word Frequency Overall=4\n",
      "Open net.html in your browser to view.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsuga\\AppData\\Local\\Temp\\ipykernel_23780\\1974943653.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = plt.cm.get_cmap(cmap_name)\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from statistics import median\n",
    "\n",
    "\n",
    "net = Network(height=\"100vh\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", cdn_resources=\"remote\", neighborhood_highlight=True)\n",
    "net.toggle_physics(True)\n",
    "\n",
    "all_words: List[str] = []\n",
    "\n",
    "# remove NONE topic where LLM could not tag the cluster for some reason\n",
    "for at in articles_topic:\n",
    "    if at.topic == \"NONE\":\n",
    "        continue\n",
    "    all_words.extend(at.corpus)\n",
    "\n",
    "shared_words: List[str] = []\n",
    "word_cluster_residency: List[str] = []\n",
    "# get filtered set of words shared within their clusters within requirements and populate the word cluster frequency dict\n",
    "for cluster in clusters:\n",
    "    cluster_words: List[str] = []\n",
    "    for idx in cluster:\n",
    "        ac = articles_corpus[idx]\n",
    "        cluster_words.extend(ac.corpus)\n",
    "    cluster_freq = FreqDist(cluster_words)\n",
    "    shared_words.extend([word for word in cluster_words if cluster_freq[word] >= len(cluster) * .75])\n",
    "    word_cluster_residency.extend(set(cluster_words))\n",
    "\n",
    "word_cluster_count = FreqDist(word_cluster_residency)\n",
    "\n",
    "topic_articles: Dict[str, List[ArticleWithTopic]] = defaultdict(lambda: [])\n",
    "# collect final articles by topic\n",
    "for article in articles_topic:\n",
    "    l = topic_articles[article.topic]\n",
    "    l.append(article)\n",
    "    topic_articles[article.topic] = l\n",
    "\n",
    "# extra word stats for debug/checking\n",
    "word_freq = FreqDist(all_words)\n",
    "mean_freq = sum(word_freq.values()) / len(articles_corpus)\n",
    "median_freq = median(word_freq.values())\n",
    "print(f\"Total Unique Words={len(set(all_words))}\")\n",
    "print(f\"Mean Word Frequency Overall={mean_freq}\")\n",
    "print(f\"Median Word Frequency Overall={median_freq}\")\n",
    "# filter words present in more than 3 clusters\n",
    "shared_words = list(set([word for word in shared_words if word_cluster_count[word] <= 3 and word in all_words]))\n",
    "# add words as nodes sizing by how many clusters they appear in\n",
    "net.add_nodes(shared_words, size=[min(max(word_cluster_count[word] * 3, 1), 10) for word in shared_words])\n",
    "\n",
    "edges: List[Tuple[str, str]] = []\n",
    "topics_n_edges: Dict[str, int] = defaultdict(lambda: 0)\n",
    "topic_colors: Dict[str, str] = {}\n",
    "# one color for each topic\n",
    "colors = get_combined_colormap(len(topic_articles.items()))\n",
    "# delete topic group \"NONE\" if it exists\n",
    "if \"NONE\" in topic_articles:\n",
    "    del topic_articles[\"NONE\"]\n",
    "i = 0\n",
    "# determine topic colors and define colored edges between topics and words\n",
    "for topic, articles in topic_articles.items():\n",
    "    color = colors[i]\n",
    "    topic_colors[topic] = color\n",
    "    i += 1\n",
    "    topic_words: Set[str] = set()\n",
    "    for article in articles:\n",
    "        topic_words = topic_words.union(article.corpus)\n",
    "    for word in topic_words:\n",
    "        if word in shared_words:\n",
    "            topics_n_edges[topic] = topics_n_edges[topic] + 1\n",
    "            edges.append((topic, word, color))\n",
    "\n",
    "# delete totally disjoint topics with no edges\n",
    "for k in list(topic_articles.keys()):\n",
    "    if topics_n_edges[k] < 1:\n",
    "        del topic_articles[k]\n",
    "\n",
    "# create topic nodes sized by relative proportion of all articles\n",
    "max_articles = max([len(k) for k in topic_articles.values()])\n",
    "for topic, articles in topic_articles.items():\n",
    "    net.add_node(topic, size=(len(articles)/max_articles) * 50 + 20, color=topic_colors[topic])\n",
    "\n",
    "# create colored edges defined above\n",
    "for k1, k2, color in edges:\n",
    "    net.add_edge(source=k1, to=k2, color=color)\n",
    "\n",
    "net.save_graph(\"net.html\")\n",
    "print(\"Open net.html in your browser to view.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Findings\n",
    "A number of things can be seen from the visual. The topics and words are familiar from recent events and the associated words are intuitive the majority of the time.\\\n",
    "Since the last 500 articles were used, the data goes back a few months and the topics are somewhat general sometimes. The largest and most connected topics are those relating to politics and economics.\\\n",
    "This section will be continued further in the final paper.\\\n",
    "Much more work can be done in this notebook but ultimately the results are readable and a lot of time has already been sunk into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
