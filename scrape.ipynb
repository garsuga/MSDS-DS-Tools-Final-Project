{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.1.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: decorator in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sentencetransformers (from versions: none)\n",
      "ERROR: No matching distribution found for sentencetransformers\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyquery in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyquery) (5.2.2)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyquery) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets\n",
    "%pip install sentencetransformers\n",
    "%pip install pyquery\n",
    "%pip install tqdm\n",
    "%pip install nltk\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "import asyncio\n",
    "from typing import Any, AsyncGenerator, List, Set, Tuple, Dict\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "TIMES_URL = \"https://time.com/\"\n",
    "TIMES_HOME_URL = urljoin(TIMES_URL, \"section/us/\")\n",
    "TIMES_PAGE_URL = \"/section/us/?page={page}\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:128.0) Gecko/20100101 Firefox/128.0\"\n",
    "}\n",
    "N_ARTICLES = 20\n",
    "SUMMARIZED_ARTICLES_PICKLE = \"summarized_articles.pkl\"\n",
    "ARTICLES_CORPUS_PICKLE = \"articles_with_corpus.pkl\"\n",
    "ARTICLES_PICKLE = \"articles.pkl\"\n",
    "ARTICLES_TOPIC_PICKLE = \"articles_with_topic.pkl\"\n",
    "CLUSTERS_PICKLE = \"clusters.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_str(cls):\n",
    "    def custom_str(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        properties = {k: v for k, v in self.__dict__.items()}\n",
    "        return json.dumps({\"class\": class_name, \"properties\": properties}, indent=4)\n",
    "    cls.__str__ = custom_str\n",
    "    cls.__repr__ = custom_str\n",
    "    return cls\n",
    "\n",
    "def remove_unicode_and_normalize(s: str) -> str:\n",
    "    normalized = unicodedata.normalize('NFKD', s)\n",
    "    return normalized.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@json_str\n",
    "class RateLimiter:\n",
    "    def __init__(self, n: int=1, interval:float=2):\n",
    "        self.n = n\n",
    "        self.interval = interval\n",
    "        self.calls = 0\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self) -> None:\n",
    "        async with self.lock:\n",
    "            if self.calls >= self.n:\n",
    "                await asyncio.sleep(self.interval)\n",
    "                self.calls = 0\n",
    "            self.calls += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "\n",
    "T = TypeVar('T')\n",
    "async def async_islice(generator: AsyncGenerator[T, None], stop: int, start: int = 0) -> AsyncGenerator[T, None]:\n",
    "    i = -1\n",
    "    async for value in generator:\n",
    "        i += 1\n",
    "        if i < start:\n",
    "            continue\n",
    "        if i >= stop:\n",
    "            break\n",
    "        yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@json_str\n",
    "class Article:\n",
    "    def __init__(self, url: str, title: str, paragraphs: List[str]):\n",
    "        self.paragraphs: List[str] = paragraphs\n",
    "        self.title: str = title\n",
    "        self.url: str = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def iter_times(max_pages: int = 10) -> AsyncGenerator[Article, None]:\n",
    "    rl = RateLimiter(interval=1)\n",
    "\n",
    "    def get_page_from_url(url) -> pq:\n",
    "        resp = requests.get(url, headers=HEADERS)\n",
    "        if resp.status_code != 200:\n",
    "            return None\n",
    "        page = pq(resp.text)\n",
    "        return page\n",
    "    \n",
    "    async def iter_pages(start_url: str) -> AsyncGenerator[pq, None]:\n",
    "        page_num = 1\n",
    "        \n",
    "        def get_next_page() -> pq:\n",
    "            nonlocal page_num\n",
    "            page_num += 1\n",
    "            page = get_page_from_url(urljoin(TIMES_URL, TIMES_PAGE_URL.format(page=page_num)))\n",
    "            return page\n",
    "        \n",
    "        await rl.acquire()\n",
    "        page = get_page_from_url(start_url)\n",
    "        await rl.acquire()\n",
    "        next_page = get_next_page()\n",
    "        yield page\n",
    "\n",
    "        while next_page is not None:\n",
    "            yield page\n",
    "            await rl.acquire()\n",
    "            page = next_page\n",
    "            next_page = get_next_page()\n",
    "\n",
    "    def get_article_urls(head_page: str) -> List[str]:\n",
    "        article_urls = [x.attr(\"href\") for x in head_page(\"div.component div.taxonomy-tout a\").items()]\n",
    "        return article_urls\n",
    "    \n",
    "    def get_article_from_url(url: str) -> Article:\n",
    "        page = get_page_from_url(url)\n",
    "        try:\n",
    "            title = remove_unicode_and_normalize(page(\"h1.self-baseline\").text())\n",
    "            paragraphs = [remove_unicode_and_normalize(x.text()) for x in page(\"p.self-baseline.px-0\").items()]\n",
    "            return Article(url, title, paragraphs)\n",
    "        except Exception as e:\n",
    "            print(f\"Article at url='{url}' failed to parse: {e}\")\n",
    "            return None\n",
    "\n",
    "    async for page in async_islice(iter_pages(TIMES_HOME_URL), max_pages):\n",
    "        urls = get_article_urls(page)\n",
    "        for url in urls:\n",
    "            yield get_article_from_url(urljoin(TIMES_URL, url))\n",
    "            await rl.acquire()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267d2083894a48f4b0408465b871d499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/500 before iterator ended.\n"
     ]
    }
   ],
   "source": [
    "N_ARTICLES = 500\n",
    "\n",
    "articles: List[Article] = []\n",
    "with tqdm(total=N_ARTICLES) as pbar:\n",
    "    async for article in async_islice(iter_times(max_pages=100), N_ARTICLES):\n",
    "        pbar.update(1)\n",
    "        if article is not None:\n",
    "            articles.append(article)\n",
    "    tqdm.write(f\"Processed {len(articles)}/{N_ARTICLES} before iterator ended.\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(ARTICLES_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(ARTICLES_PICKLE, \"rb\") as f:\n",
    "    articles: List[Article] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gsuga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "#https://stackoverflow.com/questions/43742956/fast-named-entity-removal-with-nltk\n",
    "def fast_ne_removal(tokens):\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged)\n",
    "    non_entities = [leaf[0] for leaf in tree if type(leaf) != nltk.Tree]\n",
    "    return non_entities\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    words = [word for word in words if freq_dist[word] > 1]\n",
    "    \n",
    "    words = fast_ne_removal(words)\n",
    "    words = [s.lower() for s in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@json_str\n",
    "class SummarizedArticle(Article):    \n",
    "    def __init__(self, summary: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.summary: str = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9efb99e2ca4f4b8a860461be1ffe24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 499 articles.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "# https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/text-summarization\n",
    "from LexRank import degree_centrality_scores\n",
    "\n",
    "SUMMARY_LENGTH = 3\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "summarized_articles: List[SummarizedArticle] = []\n",
    "\n",
    "with tqdm(total=len(articles)) as pbar:\n",
    "    for article in articles:\n",
    "        sentences: List[str] = []\n",
    "        for paragraph in article.paragraphs:\n",
    "            if paragraph is not None and len(paragraph) > 0:\n",
    "                sentences.extend([tok for tok in sent_tokenize(paragraph) if len(tok.strip()) > 0])\n",
    "\n",
    "        sentences = np.array(sentences)\n",
    "\n",
    "        if len(sentences) < 1:\n",
    "            continue\n",
    "        \n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/text-summarization/text-summarization.py\n",
    "        similarity_scores = model.similarity(embeddings, embeddings).numpy()\n",
    "        centrality_scores = degree_centrality_scores(similarity_scores, threshold=None)\n",
    "        most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "\n",
    "        summary = \"\\n\".join(sentences[most_central_sentence_indices[0:SUMMARY_LENGTH]])\n",
    "        summarized_articles.append(SummarizedArticle(summary, **article.__dict__))\n",
    "        pbar.update(1)\n",
    "    pbar.write(f\"Summarized {len(summarized_articles)} articles.\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(SUMMARIZED_ARTICLES_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(summarized_articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(SUMMARIZED_ARTICLES_PICKLE, \"rb\") as f:\n",
    "    summarized_articles: List[SummarizedArticle] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides breaching the Trump campaign, officials also believe that Iran tried to hack into the presidential campaign of Kamala Harris.\n",
      "We have observed increasingly aggressive Iranian activity during this election cycle, specifically involving influence operations targeting the American public and cyber operations targeting Presidential campaigns, said the statement released by the FBI, the Office of the Director of National Intelligence and the Cybersecurity and Infrastructure Security Agency.\n",
      "Earlier this month, Microsoft issued a report detailing foreign agents attempts to interfere in this years election, citing an instance of an Iranian military intelligence unit in June sending a spear-phishing email to a high-ranking official of a presidential campaign from a compromised email account of a former senior advisor.\n",
      "\n",
      "As the Democratic Party finds itself once again on the cusp of potentially electing its first female President, Clinton, now 76, used her speech to rally behind Vice President Kamala Harris, salute President Joe Biden, and remind Democrats of the progress made and the opportunities still within reach since her historic presidential run in 2016.\n",
      "Eight years ago, Hillary Clinton addressed the Democratic National Convention as the first woman to lead a major partys presidential ticket, hoping to break the proverbial glass ceiling.\n",
      "Clinton spoke with a blend of nostalgia and optimism, recounting the challenges and breakthroughs of her 2016 campaign and framing Harris campaign as the latest chapter in a continuing historical struggle for gender equality in American politics.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([s.summary for s in summarized_articles[0:2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@json_str\n",
    "class ArticleWithCorpus(SummarizedArticle):\n",
    "    def __init__(self, corpus: List[str], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.corpus : List[str] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_corpus: List[ArticleWithCorpus] = [ArticleWithCorpus(clean_text(\"\\n\".join(s.paragraphs)), **s.__dict__) for s in summarized_articles]\n",
    "\n",
    "import pickle\n",
    "with open(ARTICLES_CORPUS_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(articles_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(ARTICLES_CORPUS_PICKLE, \"rb\") as f:\n",
    "    articles_corpus: List[ArticleWithCorpus] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4974504a2b584c6d88ead18718ddb761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode([\" \".join(a.corpus) for a in articles_corpus], batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "clusters: List[List[int]] = util.community_detection(embeddings, min_community_size=3, threshold=0.5)\n",
    "\n",
    "import pickle\n",
    "with open(CLUSTERS_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(CLUSTERS_PICKLE, \"rb\") as f:\n",
    "    clusters: List[List[int]] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) 37 Articles\n",
      "\t \n",
      "\t How Republicans Are Reacting to the Colorado Ruling to Remove Trump From the Ballot\n",
      "\t Read the Full Transcripts of Donald Trumps Interviews With TIME\n",
      "\t...\n",
      "(2) 35 Articles\n",
      "\t What Americas Student Photojournalists Saw at the Campus Protests\n",
      "\t Columbias Relationship With Student Protesters Has Long Been Fraught\n",
      "\t Gaza Calls, Columbia Falls: Campus Protesters Defy Suspension Threats and Occupy Hall\n",
      "\t...\n",
      "(3) 12 Articles\n",
      "\t High Debt Threatens the U.S. Economy\n",
      "\t The Republican War on Food Programs\n",
      "\t Is America in Decline?\n",
      "\t...\n"
     ]
    }
   ],
   "source": [
    "for i, cluster in enumerate(clusters[0:3]):\n",
    "    print(f\"({i + 1}) {len(cluster)} Articles\")\n",
    "    for sentence_id in cluster[0:3]:\n",
    "        print(\"\\t\", articles_corpus[sentence_id].title)\n",
    "    print(\"\\t...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "modelspec='''\n",
    "FROM llama3.1:70b\n",
    "SYSTEM You will be given a number of article summaries you need to find the common topic between. This topic should only be a couple of words at a maximum and there should only be one topic. Make sure the topic corresponds to EVERY summary provided. If you cannot find a topic respond with only NONE. Summaries are separated by newlines. Respond only with the short topic as described with no extra information.\n",
    "PARAMETER temperature 0.0\n",
    "'''\n",
    "\n",
    "ollama.create(model='llama3.1-70-topic', modelfile=modelspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@json_str\n",
    "class ArticleWithTopic(ArticleWithCorpus):\n",
    "    def __init__(self, topic: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.topic: str = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsuga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaf92e0b7df40e3b935bdad6b79fda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing Clusters:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 54 clusters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196ce2eba1344851ad7642a43198bd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting topics:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "# https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/text-summarization\n",
    "from LexRank import degree_centrality_scores\n",
    "\n",
    "topic_sentences: List[List[str]] = []\n",
    "for cluster in clusters:\n",
    "    sentences: List[str] = []\n",
    "    for idx in cluster:\n",
    "        sentences.extend(sent_tokenize(articles_corpus[idx].summary))\n",
    "    topic_sentences.append(sentences)\n",
    "\n",
    "TOPIC_SUMMARY_LENGTH = 10\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "summaries: List[str] = []\n",
    "with tqdm(total=len(clusters), desc=\"Summarizing Clusters\") as pbar:\n",
    "    for i, sentences in enumerate(topic_sentences):\n",
    "        sentences = np.array(sentences)\n",
    "        if len(sentences) < 1:\n",
    "            continue\n",
    "        \n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/text-summarization/text-summarization.py\n",
    "        similarity_scores = model.similarity(embeddings, embeddings).numpy()\n",
    "        centrality_scores = degree_centrality_scores(similarity_scores, threshold=None)\n",
    "        most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "\n",
    "        summary = \"\\n\\n\".join(sentences[most_central_sentence_indices[0:TOPIC_SUMMARY_LENGTH]])\n",
    "        summaries.append(summary)\n",
    "        pbar.update(1)\n",
    "    pbar.write(f\"Summarized {len(summaries)} clusters.\")\n",
    "\n",
    "\n",
    "\n",
    "#summaries = [\"\\n\\n\\n\".join([articles_corpus[sentence_id].summary.replace(\"\\n\", \" \") for sentence_id in cluster[0:3]]) for cluster in clusters]\n",
    "\n",
    "articles_topic: List[ArticleWithTopic] = []\n",
    "with tqdm(total=len(summaries), desc=\"Extracting topics\") as pbar:\n",
    "    for i, summary in enumerate(summaries):\n",
    "        topic = ollama.generate(model=\"llama3.1-70-topic\", prompt=summary)['response']\n",
    "        cluster = clusters[i]\n",
    "        \n",
    "        for sentence_id in cluster:\n",
    "            articles_topic.append(ArticleWithTopic(topic, **articles_corpus[sentence_id].__dict__))\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "import pickle\n",
    "with open(ARTICLES_TOPIC_PICKLE, \"wb\") as f:\n",
    "    pickle.dump(articles_topic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(ARTICLES_TOPIC_PICKLE, \"rb\") as f:\n",
    "    articles_topic: List[ArticleWithTopic] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from pyvis) (8.25.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyvis) (3.2.2)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyvis) (3.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=5.3.0->pyvis) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\gsuga\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\gsuga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import rgb2hex\n",
    "\n",
    "#chatgpt\n",
    "def get_combined_colormap(N):\n",
    "    colormaps = ['tab20', 'tab20c', 'tab10', 'Set3']  # A combination of discrete colormaps\n",
    "    colors = []\n",
    "\n",
    "    for cmap_name in colormaps:\n",
    "        cmap = plt.cm.get_cmap(cmap_name)\n",
    "        colors.extend([rgb2hex(c) for c in cmap.colors])\n",
    "\n",
    "    return colors[:N] if N <= len(colors) else colors * (N // len(colors)) + colors[:N % len(colors)]\n",
    "    #return [colors[i % len(colors)] for i in range(0, N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words=5081\n",
      "Mean Word Frequency Overall=178.82565130260522\n",
      "Median Word Frequency Overall=4\n",
      "Open net.html in your browser to view.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsuga\\AppData\\Local\\Temp\\ipykernel_21612\\241976220.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = plt.cm.get_cmap(cmap_name)\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from statistics import median\n",
    "\n",
    "net = Network(height=\"100vh\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", cdn_resources=\"remote\", neighborhood_highlight=True)\n",
    "net.toggle_physics(True)\n",
    "\n",
    "all_words: List[str] = []\n",
    "\n",
    "for at in articles_topic:\n",
    "    if at.topic == \"NONE\":\n",
    "        continue\n",
    "    all_words.extend(at.corpus)\n",
    "\n",
    "shared_words: List[str] = []\n",
    "word_cluster_residency: List[str] = []\n",
    "for cluster in clusters:\n",
    "    cluster_words: List[str] = []\n",
    "    for idx in cluster:\n",
    "        ac = articles_corpus[idx]\n",
    "        cluster_words.extend(ac.corpus)\n",
    "    cluster_freq = FreqDist(cluster_words)\n",
    "    shared_words.extend([word for word in cluster_words if cluster_freq[word] >= len(cluster) * .75])\n",
    "    word_cluster_residency.extend(set(cluster_words))\n",
    "\n",
    "word_cluster_count = FreqDist(word_cluster_residency)\n",
    "\n",
    "topic_articles: Dict[str, List[ArticleWithTopic]] = defaultdict(lambda: [])\n",
    "for article in articles_topic:\n",
    "    l = topic_articles[article.topic]\n",
    "    l.append(article)\n",
    "    topic_articles[article.topic] = l\n",
    "\n",
    "word_freq = FreqDist(all_words)\n",
    "mean_freq = sum(word_freq.values()) / len(articles_corpus)\n",
    "median_freq = median(word_freq.values())\n",
    "print(f\"Total Unique Words={len(set(all_words))}\")\n",
    "print(f\"Mean Word Frequency Overall={mean_freq}\")\n",
    "print(f\"Median Word Frequency Overall={median_freq}\")\n",
    "shared_words = list(set([word for word in shared_words if word_cluster_count[word] <= 3 and word in all_words]))\n",
    "\n",
    "net.add_nodes(shared_words, size=[min(max(word_cluster_count[word], 1), 5) for word in shared_words])\n",
    "\n",
    "edges: List[Tuple[str, str]] = []\n",
    "colors = get_combined_colormap(len(topic_articles.items()))\n",
    "if \"NONE\" in topic_articles:\n",
    "    del topic_articles[\"NONE\"]\n",
    "i = 0\n",
    "for topic, articles in topic_articles.items():\n",
    "    color = colors[i]\n",
    "    i += 1\n",
    "    topic_words: Set[str] = set()\n",
    "    for article in articles:\n",
    "        topic_words = topic_words.union(article.corpus)\n",
    "    for word in topic_words:\n",
    "        if word in shared_words:\n",
    "            edges.append((topic, word, color))\n",
    "\n",
    "net.add_nodes(list(topic_articles.keys()), size=[30] * len(topic_articles.keys()), color=[colors[i] for i in range(0, len(topic_articles.items()))])\n",
    "for k1, k2, color in edges:\n",
    "    net.add_edge(source=k1, to=k2, color=color)\n",
    "\n",
    "net.save_graph(\"net.html\")\n",
    "print(\"Open net.html in your browser to view.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
